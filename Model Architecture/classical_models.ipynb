{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely import wkt\n",
    "from shapely.affinity import translate, scale\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "file_path = '../../Desktop/Matrix/DATA/'\n",
    "\n",
    "df_sample = pd.read_parquet(file_path + \"detailed_woning_type_sample.parquet\")\n",
    "df = pd.read_csv(file_path + \"bag_image_summary.csv\", dtype=\"string\")\n",
    "df_joined = pd.merge(df_sample, df, how=\"left\", right_on=\"bag_id\", left_on=\"bag_nummeraanduidingid\")\n",
    "df_sample_with_urls = df_joined[df_joined[\"frontview_exists\"].notna()]\n",
    "\n",
    "# If you want to add the file path to the URLs, set this to True\n",
    "add_file_path_to_urls = False\n",
    "\n",
    "# Currently a funda sourced Url goes from: \n",
    "# frontview/0797/2000/0002/3888/0797200000023888.jpg\n",
    "# to: img_dataset/07/079720000002-funda.jpg\n",
    "def extract_path(url, source):\n",
    "    if pd.isna(url) or url == '' or url is None:\n",
    "        return ''\n",
    "    id = url.rstrip('/').split('/')[-1]\n",
    "    id, *_ = id.split('.')\n",
    "    m = re.match(r'(\\d{2})', id)\n",
    "    first_two_digits = m.group(1) if m else ''\n",
    "    return f\"img_dataset/{first_two_digits}/{id}-{source}.jpg\"\n",
    "\n",
    "link_cols = ['frontview_funda_url', 'frontview_google_url', 'frontview_funda_in_business_url']\n",
    "link_sources = ['funda', 'google', 'funda-in-business'] # Sources are in file name, so need to be added to filename for correct name\n",
    "\n",
    "for col, source in zip(link_cols, link_sources):\n",
    "    df_sample_with_urls[f'{col}_split'] = df_sample_with_urls[col].map(lambda url: extract_path(url, source))\n",
    "\n",
    "# If you want to add the file path to the URLs, set add_file_path_to_urls to True\n",
    "if add_file_path_to_urls:\n",
    "    df_sample_with_urls[[f'{col}_split' for col in link_cols]] = df_sample_with_urls[[f'{col}_split' for col in link_cols]].map(lambda x: file_path + x if x else '')\n",
    "    add_file_path_to_urls = False\n",
    "\n",
    "df_sample_with_urls.to_csv(\n",
    "    file_path + \"Full_preprocessed_detailed_house.csv\",\n",
    "    index=False,\n",
    "    encoding='utf-8',\n",
    ")\n",
    "\n",
    "df = pd.read_csv(file_path + \"Full_preprocessed_detailed_house.csv\", dtype=\"string\")\n",
    "\n",
    "# Verschillend is a special case, so we remove it from the dataset\n",
    "df = df[df['build_type'] != 'Verschillend']\n",
    "\n",
    "def pick_first_url(row):\n",
    "    for col in [f\"{c}_split\" for c in link_cols]:\n",
    "        val = row[col]\n",
    "        if pd.notna(val) and val != '':\n",
    "            return val\n",
    "    return ''\n",
    "\n",
    "df['frontview_url'] = df.apply(pick_first_url, axis=1)\n",
    "df = df[df['frontview_url'] != '']\n",
    "\n",
    "# Ensure 'opp_pand' and 'oppervlakte' are numeric before division\n",
    "df['procent_ingenomen'] = pd.to_numeric(df['opp_pand'], errors='coerce') / pd.to_numeric(df['oppervlakte'], errors='coerce')\n",
    "\n",
    "df['huisnr_bag_letter'] = df['huisnr_bag_letter'].notna().astype(int)\n",
    "df['huisnr_bag_toevoeging'] = df['huisnr_bag_toevoeging'].notna().astype(int)\n",
    "\n",
    "df['is_monument'] = df['is_monument'].fillna(0).astype(int)\n",
    "df['is_protected'] = df['is_protected'].fillna(0).astype(int)\n",
    "\n",
    "df = df.drop(columns=['bag_nummeraanduidingid', 'frontview_exists', 'random_rank', 'num_funda_images',\n",
    "                      'frontview_funda_url', 'frontview_google_url', 'frontview_funda_in_business_url', \n",
    "                      'frontview_funda_url_split', 'frontview_google_url_split', 'frontview_funda_in_business_url_split',\n",
    "                      'special_house_type', 'source_data_result_id',\n",
    "                      'straatnaam', 'postcode', 'plaatsnaam', 'source_data_timestamp', 'bag_id', 'frontview_url', 'oppervlakte'\n",
    "                      ])\n",
    "\n",
    "# Oversample corriderflat door random te dupliceren\n",
    "merge_map = {\n",
    "    'Bovenwoning': 'Bovenwoning/Benedenwoning/Maisonette',\n",
    "    'Benedenwoning': 'Bovenwoning/Benedenwoning/Maisonette',\n",
    "    'Maisonnette': 'Bovenwoning/Benedenwoning/Maisonette',\n",
    "    'Corridorflat': 'Corridorflat/Galerijflat',\n",
    "    'Galerijflat': 'Corridorflat/Galerijflat',\n",
    "    'Hoekwoning': 'Hoekwoning/Eindwoning',\n",
    "    'Eindwoning': 'Hoekwoning/Eindwoning',\n",
    "    'Portiekflat': 'Portiekflat/Portiekwoning',\n",
    "    'Portiekwoning': 'Portiekflat/Portiekwoning'\n",
    "\n",
    "    # etc.\n",
    "}\n",
    "\n",
    "df['woningtype'] = df['woningtype'].map(merge_map).fillna(df['woningtype'])\n",
    "\n",
    "# Full preprocessed dataset with URLS, can be loaded into pipeline.\n",
    "df.to_csv(\n",
    "    file_path + \"Full_preprocessed_detailed_house.csv\",\n",
    "    index=False,\n",
    "    encoding='utf-8',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_orientation(polygon):\n",
    "    if polygon.is_empty or not polygon.is_valid:\n",
    "        return np.nan\n",
    "\n",
    "    # Get minimum rotated rectangle\n",
    "    mrr = polygon.minimum_rotated_rectangle\n",
    "    coords = list(mrr.exterior.coords)\n",
    "\n",
    "    # Find the longest edge\n",
    "    max_length = 0\n",
    "    angle = 0\n",
    "\n",
    "    for i in range(len(coords) - 1):\n",
    "        p1 = coords[i]\n",
    "        p2 = coords[i + 1]\n",
    "\n",
    "        dx = p2[0] - p1[0]\n",
    "        dy = p2[1] - p1[1]\n",
    "\n",
    "        length = np.hypot(dx, dy)\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "            angle = np.degrees(np.arctan2(dy, dx))\n",
    "\n",
    "    # Normalize angle to 0â€“180\n",
    "    return angle % 180\n",
    "\n",
    "def compute_elongation(polygon):\n",
    "    if polygon.is_empty or not polygon.is_valid:\n",
    "        return np.nan\n",
    "\n",
    "    # Minimum rotated rectangle (oriented bounding box)\n",
    "    min_rect = polygon.minimum_rotated_rectangle\n",
    "\n",
    "    # Get corner points of the box\n",
    "    coords = list(min_rect.exterior.coords)\n",
    "\n",
    "    # Compute distances between the 4 sides\n",
    "    edge_lengths = [np.linalg.norm(np.subtract(coords[i], coords[i + 1])) for i in range(4)]\n",
    "\n",
    "    width = min(edge_lengths)\n",
    "    height = max(edge_lengths)\n",
    "\n",
    "    if height == 0:  # Prevent divide-by-zero\n",
    "        return np.nan\n",
    "\n",
    "    return width / height\n",
    "\n",
    "def rasterize_polygon(geom, size=224):\n",
    "    bounds = geom.bounds\n",
    "    geom = translate(geom, xoff=-bounds[0], yoff=-bounds[1])\n",
    "    scale_x = size / (bounds[2] - bounds[0] + 1e-8)\n",
    "    scale_y = size / (bounds[3] - bounds[1] + 1e-8)\n",
    "    geom = scale(geom, xfact=scale_x, yfact=scale_y, origin=(0, 0))\n",
    "\n",
    "    img = Image.new(\"L\", (size, size), 0)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    coords = [(x, size - y) for x, y in geom.exterior.coords]\n",
    "    draw.polygon(coords, outline=1, fill=1)\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['geometry_wkt'] = df['geometry'].apply(wkt.loads)\n",
    "\n",
    "df['centroid_x'] = df['geometry_wkt'].apply(lambda geom: geom.centroid.x)\n",
    "df['centroid_y'] = df['geometry_wkt'].apply(lambda geom: geom.centroid.y)\n",
    "\n",
    "df['area'] = df['geometry_wkt'].apply(lambda geom: geom.area)\n",
    "\n",
    "# perimeter = Sum of the lengths of all edges forming the boundary of a polygon\n",
    "df['perimeter'] = df['geometry_wkt'].apply(lambda g: g.length)\n",
    "\n",
    "# Gives 1 for a perfect circle (most compact shape)\n",
    "# Gets closer to 0 for long, skinny, jagged shapes\n",
    "df['compactness'] = (\n",
    "    4 * np.pi * df['area'] / (df['perimeter'] ** 2)\n",
    ")\n",
    "\n",
    "df['num_vertices'] = df['geometry_wkt'].apply(lambda g: len(g.exterior.coords))\n",
    "\n",
    "df['elongation'] = df['geometry_wkt'].apply(compute_elongation)\n",
    "\n",
    "df['orientation_deg'] = df['geometry_wkt'].apply(compute_orientation)\n",
    "\n",
    "df['num_vertices_log'] = np.log1p(df['num_vertices'])\n",
    "\n",
    "df.drop(columns=[\"geometry\", \"geometry_wkt\"], inplace=True)\n",
    "\n",
    "\n",
    "df.to_csv(\n",
    "    file_path + \"Full_preprocessed_detailed_house.csv\",\n",
    "    index=False,\n",
    "    encoding='utf-8',\n",
    ")\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "features = [\n",
    "    'huisnr',                                   # 0 - inf\n",
    "    'huisnr_bag_letter',                        # 0 - 1\n",
    "    'huisnr_bag_toevoeging',                    # 0 - 1\n",
    "    'opp_pand',                                 # StandardScaler\n",
    "    'oppervlakte',                              # StandardScaler\n",
    "    'build_year',                               # StandardScaler\n",
    "    'build_type_Appartement',                   # OneHotEncoder\n",
    "    'build_type_Hoekwoning',                    # OneHotEncoder\n",
    "    'build_type_Tussen_of_geschakelde_woning',   # OneHotEncoder\n",
    "    'build_type_Tweeonder1kap',                 # OneHotEncoder\n",
    "    'build_type_Verschillend',                  # OneHotEncoder\n",
    "    'is_monument',                              # 0 - 1\n",
    "    'is_protected',                             # 0 - 1\n",
    "    'procent_ingenomen'                         # 0 - 1\n",
    "]\n",
    "\n",
    "target = 'woningtype'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huisnr                                     float64\n",
      "huisnr_bag_letter                            int64\n",
      "huisnr_bag_toevoeging                        int64\n",
      "opp_pand                                   float64\n",
      "build_year                                 float64\n",
      "is_monument                                  int64\n",
      "is_protected                                 int64\n",
      "procent_ingenomen                          float64\n",
      "centroid_x                                 float64\n",
      "centroid_y                                 float64\n",
      "area                                       float64\n",
      "perimeter                                  float64\n",
      "compactness                                float64\n",
      "num_vertices                                 int64\n",
      "elongation                                 float64\n",
      "orientation_deg                            float64\n",
      "num_vertices_log                           float64\n",
      "build_type_Appartement                     float64\n",
      "build_type_Hoekwoning                      float64\n",
      "build_type_Tussen_of_geschakelde_woning    float64\n",
      "build_type_Tweeonder1kap                   float64\n",
      "build_type_Vrijstaande_woning              float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "numeric_cols = ['opp_pand', 'build_year', 'huisnr', 'procent_ingenomen']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_df[numeric_cols] = scaler.fit_transform(train_df[numeric_cols])\n",
    "val_df[numeric_cols] = scaler.transform(val_df[numeric_cols])\n",
    "test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\n",
    "\n",
    "\n",
    "for col in ['centroid_x', 'centroid_y']:\n",
    "    min_val = train_df[col].min()\n",
    "    max_val = train_df[col].max()\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df[col] = (df[col] - min_val) / (max_val - min_val + 1e-8)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoder.fit(train_df[['build_type']])\n",
    "feature_names = [name.replace(' ', '_') for name in encoder.get_feature_names_out(['build_type'])]\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    encoded_array = encoder.transform(df[['build_type']])\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=feature_names, index=df.index)\n",
    "    df.drop(columns='build_type', inplace=True)\n",
    "    df[encoded_df.columns] = encoded_df\n",
    "\n",
    "\n",
    "\n",
    "X_train = train_df.drop(columns=[\"woningtype\"])\n",
    "print(X_train.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4823\n",
      "Precision (macro): 0.5074\n",
      "Recall (macro): 0.4803\n",
      "F1 Score (macro): 0.4453\n",
      "Cohen's Kappa: 0.4134\n",
      "Log Loss: 3.1497\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#classical naive bayas probability model for baseline\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "# Suppose 'col_name' is your column with number-strings\n",
    "\n",
    "X_train = train_df.drop(columns=[\"woningtype\"])\n",
    "y_train = train_df[\"woningtype\"]\n",
    "\n",
    "X_val = val_df.drop(columns=[\"woningtype\"])\n",
    "y_val = val_df[\"woningtype\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"woningtype\"])\n",
    "y_test = test_df[\"woningtype\"]\n",
    "\n",
    "#wow moeilijke code (niet)\n",
    "#guassian NB ipv categorical omdat onze data niet alleen categorical is (WOW)\n",
    "NB = GaussianNB()\n",
    "NB.fit(X_train, y_train)\n",
    "from sklearn.metrics import (\n",
    "        accuracy_score, precision_score, recall_score, f1_score,\n",
    "        log_loss, cohen_kappa_score, \n",
    "    )\n",
    "\n",
    "y_pred = NB.predict(X_val)\n",
    "y_proba = NB.predict_proba(X_val)\n",
    "    \n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision = precision_score(y_val, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_val, y_pred, average='macro', zero_division=0)\n",
    "f1 = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
    "kappa = cohen_kappa_score(y_val, y_pred)\n",
    "logloss = log_loss(y_val, y_proba)\n",
    "    \n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision (macro): {precision:.4f}\")\n",
    "print(f\"Recall (macro): {recall:.4f}\")\n",
    "print(f\"F1 Score (macro): {f1:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "print(f\"Log Loss: {logloss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6508\n",
      "Precision (macro): 0.6425\n",
      "Recall (macro): 0.6462\n",
      "F1 Score (macro): 0.6385\n",
      "Cohen's Kappa: -0.0010\n",
      "Log Loss: 1.2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#discriminitive classical model (probabky xgboost or random forest)\n",
    "#random forest is denk beter als deze modellen alleen als baseline worden gebruikt\n",
    "#xg is meer accurate tho\n",
    "#heb de categorical features niet geencode want dacht dat koen dat al gedaan had? anders ga ik er nog achteraan\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train = train_df.drop(columns=[\"woningtype\"])\n",
    "y_train = train_df[\"woningtype\"]\n",
    "\n",
    "X_val = val_df.drop(columns=[\"woningtype\"])\n",
    "y_val = val_df[\"woningtype\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"woningtype\"])\n",
    "y_test = test_df[\"woningtype\"]\n",
    "\n",
    "#random forest is minder accuraat maar meer interpeteerbaar en minder complex\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#accurater maar moet ff meer hyperparamter tuningg doen\n",
    "y_pred = rf.predict(X_test)\n",
    "y_proba = rf.predict_proba(X_test)\n",
    "    \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "kappa = cohen_kappa_score(y_val, y_pred)\n",
    "logloss = log_loss(y_test, y_proba)\n",
    "    \n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision (macro): {precision:.4f}\")\n",
    "print(f\"Recall (macro): {recall:.4f}\")\n",
    "print(f\"F1 Score (macro): {f1:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "print(f\"Log Loss: {logloss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
