{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4f4dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    cohen_kappa_score,\n",
    "    log_loss,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely import wkt\n",
    "from shapely.affinity import translate, scale\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from shapely.affinity import translate, scale\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "file_path = os.getenv('FILE_PATH')\n",
    "\n",
    "df_sample = pd.read_parquet(file_path + \"detailed_woning_type_sample.parquet\")\n",
    "df = pd.read_csv(file_path + \"bag_image_summary.csv\", dtype=\"string\")\n",
    "df_joined = pd.merge(df_sample, df, how=\"left\", right_on=\"bag_id\", left_on=\"bag_nummeraanduidingid\")\n",
    "df = df_joined[df_joined[\"frontview_exists\"].notna()]\n",
    "\n",
    "# If you want to add the file path to the URLs, set this to True\n",
    "add_file_path_to_urls = True\n",
    "\n",
    "# Currently a funda sourced Url goes from: \n",
    "# frontview/0797/2000/0002/3888/0797200000023888.jpg\n",
    "# to: img_dataset/07/079720000002-funda.jpg\n",
    "def extract_path(url, source):\n",
    "    if pd.isna(url) or url == '' or url is None:\n",
    "        return ''\n",
    "    id = url.rstrip('/').split('/')[-1]\n",
    "    id, *_ = id.split('.')\n",
    "    m = re.match(r'(\\d{2})', id)\n",
    "    first_two_digits = m.group(1) if m else ''\n",
    "    return f\"img_dataset/{first_two_digits}/{id}-{source}.jpg\"\n",
    "\n",
    "link_cols = ['frontview_funda_url', 'frontview_google_url', 'frontview_funda_in_business_url']\n",
    "link_sources = ['funda', 'google', 'funda-in-business'] # Sources are in file name, so need to be added to filename for correct name\n",
    "\n",
    "for col, source in zip(link_cols, link_sources):\n",
    "    df[f'{col}_split'] = df[col].map(lambda url: extract_path(url, source))\n",
    "\n",
    "# If you want to add the file path to the URLs, set add_file_path_to_urls to True\n",
    "if add_file_path_to_urls:\n",
    "    df[[f'{col}_split' for col in link_cols]] = df[[f'{col}_split' for col in link_cols]].map(lambda x: file_path + x if x else '')\n",
    "    add_file_path_to_urls = False\n",
    "\n",
    "df.to_csv(\n",
    "    file_path + \"Full_preprocessed_detailed_house.csv\",\n",
    "    index=False,\n",
    "    encoding='utf-8',\n",
    ")\n",
    "\n",
    "df = pd.read_csv(file_path + \"Full_preprocessed_detailed_house.csv\", dtype=\"string\")\n",
    "\n",
    "# Verschillend is a special case, so we remove it from the dataset\n",
    "df = df[df['build_type'] != 'Verschillend']\n",
    "\n",
    "def pick_first_url(row):\n",
    "    for col in [f\"{c}_split\" for c in link_cols]:\n",
    "        val = row[col]\n",
    "        if pd.notna(val) and val != '':\n",
    "            return val\n",
    "    return ''\n",
    "\n",
    "df['frontview_url'] = df.apply(pick_first_url, axis=1)\n",
    "df = df[df['frontview_url'] != '']\n",
    "\n",
    "# Ensure 'opp_pand' and 'oppervlakte' are numeric before division\n",
    "df['procent_ingenomen'] = pd.to_numeric(df['opp_pand'], errors='coerce') / pd.to_numeric(df['oppervlakte'], errors='coerce')\n",
    "\n",
    "df['huisnr_bag_letter'] = df['huisnr_bag_letter'].notna().astype(int)\n",
    "df['huisnr_bag_toevoeging'] = df['huisnr_bag_toevoeging'].notna().astype(int)\n",
    "\n",
    "df['is_monument'] = df['is_monument'].fillna(0).astype(int)\n",
    "df['is_protected'] = df['is_protected'].fillna(0).astype(int)\n",
    "\n",
    "df = df.drop(columns=['bag_nummeraanduidingid', 'frontview_exists', 'random_rank', 'num_funda_images',\n",
    "                      'frontview_funda_url', 'frontview_google_url', 'frontview_funda_in_business_url', \n",
    "                      'frontview_funda_url_split', 'frontview_google_url_split', 'frontview_funda_in_business_url_split',\n",
    "                      'special_house_type', 'source_data_result_id',\n",
    "                      'straatnaam', 'postcode', 'plaatsnaam', 'source_data_timestamp', 'bag_id'\n",
    "                      ])\n",
    "\n",
    "# merge_map = {\n",
    "#     # Corridor or gallery flats\n",
    "#     'Corridorflat': 'Corridor/Galerijflat',\n",
    "#     'Galerijflat':  'Corridor/Galerijflat',\n",
    "#     'Halfvrijstaande woning': 'Halfvrijstaande woning/2-onder-1-kapwoning',\n",
    "#     '2-onder-1-kapwoning': 'Halfvrijstaande woning/2-onder-1-kapwoning',\n",
    "#     'Hoekwoning':    'Hoek/Eindwoning',\n",
    "#     'Eindwoning':    'Hoek/Eindwoning',\n",
    "# }\n",
    "\n",
    "# df['woningtype'] = df['woningtype'].map(merge_map).fillna(df['woningtype'])\n",
    "\n",
    "# Full preprocessed dataset with URLS, can be loaded into pipeline.\n",
    "df.to_csv(\n",
    "    file_path + \"Full_preprocessed_detailed_house.csv\",\n",
    "    index=False,\n",
    "    encoding='utf-8',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e25336f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_orientation(polygon):\n",
    "    if polygon.is_empty or not polygon.is_valid:\n",
    "        return np.nan\n",
    "\n",
    "    # Get minimum rotated rectangle\n",
    "    mrr = polygon.minimum_rotated_rectangle\n",
    "    coords = list(mrr.exterior.coords)\n",
    "\n",
    "    # Find the longest edge\n",
    "    max_length = 0\n",
    "    angle = 0\n",
    "\n",
    "    for i in range(len(coords) - 1):\n",
    "        p1 = coords[i]\n",
    "        p2 = coords[i + 1]\n",
    "\n",
    "        dx = p2[0] - p1[0]\n",
    "        dy = p2[1] - p1[1]\n",
    "\n",
    "        length = np.hypot(dx, dy)\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "            angle = np.degrees(np.arctan2(dy, dx))\n",
    "\n",
    "    # Normalize angle to 0â€“180\n",
    "    return angle % 180\n",
    "\n",
    "def compute_elongation(polygon):\n",
    "    if polygon.is_empty or not polygon.is_valid:\n",
    "        return np.nan\n",
    "\n",
    "    # Minimum rotated rectangle (oriented bounding box)\n",
    "    min_rect = polygon.minimum_rotated_rectangle\n",
    "\n",
    "    # Get corner points of the box\n",
    "    coords = list(min_rect.exterior.coords)\n",
    "\n",
    "    # Compute distances between the 4 sides\n",
    "    edge_lengths = [np.linalg.norm(np.subtract(coords[i], coords[i + 1])) for i in range(4)]\n",
    "\n",
    "    width = min(edge_lengths)\n",
    "    height = max(edge_lengths)\n",
    "\n",
    "    if height == 0:  # Prevent divide-by-zero\n",
    "        return np.nan\n",
    "\n",
    "    return width / height\n",
    "\n",
    "def rasterize_polygon(geom, size=224):\n",
    "    bounds = geom.bounds\n",
    "    geom = translate(geom, xoff=-bounds[0], yoff=-bounds[1])\n",
    "    scale_x = size / (bounds[2] - bounds[0] + 1e-8)\n",
    "    scale_y = size / (bounds[3] - bounds[1] + 1e-8)\n",
    "    geom = scale(geom, xfact=scale_x, yfact=scale_y, origin=(0, 0))\n",
    "\n",
    "    img = Image.new(\"L\", (size, size), 0)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    coords = [(x, size - y) for x, y in geom.exterior.coords]\n",
    "    draw.polygon(coords, outline=1, fill=1)\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8706f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "\n",
    "df['centroid_x'] = df['geometry'].apply(lambda geom: geom.centroid.x)\n",
    "df['centroid_y'] = df['geometry'].apply(lambda geom: geom.centroid.y)\n",
    "\n",
    "df['area'] = df['geometry'].apply(lambda geom: geom.area)\n",
    "\n",
    "# perimeter = Sum of the lengths of all edges forming the boundary of a polygon\n",
    "df['perimeter'] = df['geometry'].apply(lambda g: g.length)\n",
    "\n",
    "# Gives 1 for a perfect circle (most compact shape)\n",
    "# Gets closer to 0 for long, skinny, jagged shapes\n",
    "df['compactness'] = (\n",
    "    4 * np.pi * df['area'] / (df['perimeter'] ** 2)\n",
    ")\n",
    "\n",
    "df['num_vertices'] = df['geometry'].apply(lambda g: len(g.exterior.coords))\n",
    "\n",
    "df['elongation'] = df['geometry'].apply(compute_elongation)\n",
    "\n",
    "df['orientation_deg'] = df['geometry'].apply(compute_orientation)\n",
    "\n",
    "df['num_vertices_log'] = np.log1p(df['num_vertices'])\n",
    "\n",
    "df[\"mask\"] = df[\"geometry\"].apply(lambda g: rasterize_polygon(g, size=224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be3563ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize centroid_x and centroid_y\n",
    "for col in ['centroid_x', 'centroid_y']:\n",
    "    min_val = df[col].min()\n",
    "    max_val = df[col].max()\n",
    "    df[col] = df[col] / 3000000\n",
    "\n",
    "# Normalize orientation_deg\n",
    "df['orientation_deg'] = df['orientation_deg'] / 360\n",
    "\n",
    "df = df.drop(columns=['geometry', 'num_vertices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8743812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_final_data(df, base_path):\n",
    "#     \"\"\"Adds the full image path and encodes the 'woningtype' label.\"\"\"\n",
    "#     # Build full image path\n",
    "#     df['img_path'] = df['frontview_url'].apply(lambda x: os.path.join(base_path, x))\n",
    "#     return df\n",
    "\n",
    "# df = prepare_final_data(df, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "803b9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing and feature engineering are applied to the dataset separately for training, validation, and testing.\n",
    "# This is to ensure that the model does not learn from the validation and test sets during training.\n",
    "\n",
    "# adjust random_state for reproducibility\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check if standardization is needed for huisnr and pocent_ingenomen, large numbers can be encoded as categorical?\n",
    "for dataframe in [train_df, val_df, test_df]:\n",
    "    for col in ['area']:\n",
    "        scaler = StandardScaler()\n",
    "        dataframe[col] = scaler.fit_transform(dataframe[[col]])\n",
    "\n",
    "    if dataframe is train_df:\n",
    "        build_type_train = train_df[['build_type']]\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoder.fit(build_type_train)\n",
    "        raw_feature_names = encoder.get_feature_names_out(['build_type'])\n",
    "        clean_feature_names = [name.replace(' ', '_') for name in raw_feature_names]\n",
    "\n",
    "    build_type = dataframe[['build_type']]\n",
    "    encoded_array = encoder.transform(build_type)\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=clean_feature_names, index=dataframe.index)\n",
    "    dataframe.drop('build_type', axis=1, inplace=True)\n",
    "    dataframe[encoded_df.columns] = encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82f81024",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    # BAG\n",
    "    'area',\n",
    "    'centroid_x',                               # 0 - 1\n",
    "    'centroid_y',                               # 0 - 1\n",
    "    'perimeter',                                # 0 - 1\n",
    "    'compactness',                              # 0 - 1\n",
    "    'elongation',                               # 0 - 1\n",
    "    'orientation_deg',                          # 0 - 1\n",
    "    'num_vertices_log',                         # 0 - 7 (can be inf)\n",
    "]\n",
    "\n",
    "target = 'woningtype'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a38fa180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train_df.csv, val_df.csv and test_df.csv to: ../../Data/\n"
     ]
    }
   ],
   "source": [
    "train_df.to_csv(os.path.join(file_path, \"train_df.csv\"), index=False)\n",
    "val_df.to_csv(os.path.join(file_path, \"val_df.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(file_path, \"test_df.csv\"), index=False)\n",
    "\n",
    "print(\"Saved train_df.csv, val_df.csv and test_df.csv to:\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c824d9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5648"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "943bcd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woningtype\n",
      "huisnr\n",
      "huisnr_bag_letter\n",
      "huisnr_bag_toevoeging\n",
      "opp_pand\n",
      "oppervlakte\n",
      "build_year\n",
      "build_type\n",
      "is_monument\n",
      "is_protected\n",
      "frontview_url\n",
      "procent_ingenomen\n",
      "centroid_x\n",
      "centroid_y\n",
      "area\n",
      "perimeter\n",
      "compactness\n",
      "elongation\n",
      "orientation_deg\n",
      "num_vertices_log\n",
      "mask\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff1287aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Dataset\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.features = features\n",
    "        self.transform = transform\n",
    "        self.counter = 0\n",
    "\n",
    "        self.masks = [torch.tensor(m, dtype=torch.float32).unsqueeze(0) for m in df[\"mask\"]]\n",
    "        self.tabular = torch.tensor(df[features].values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(df[\"woningtype_encoded\"].values, dtype=torch.long)\n",
    "        \n",
    "        self._remove_missing()\n",
    "\n",
    "\n",
    "    def _remove_missing(self):\n",
    "        to_drop = []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            try:\n",
    "                Image.open(row['frontview_url'])\n",
    "            except Exception as e:\n",
    "                to_drop.append(idx)\n",
    "                self.counter += 1\n",
    "                print(f'Dropped {self.counter}th row due to image load error: {e}')\n",
    "        \n",
    "        # Drop all invalid rows at once\n",
    "        self.df = self.df.drop(index=to_drop).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Get mask and tabular\n",
    "        mask = self.masks[idx]\n",
    "        tabular = self.tabular[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load image\n",
    "        try:\n",
    "            img = Image.open(row['frontview_url']).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            else:\n",
    "                img = torch.tensor(np.array(img), dtype=torch.float32).permute(2, 0, 1) / 255.0  # normalize manually\n",
    "        except Exception as e:\n",
    "            # Handle failed image read by recursively calling next index\n",
    "            print(e)\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "        return img, mask, tabular, label\n",
    "\n",
    "# Model\n",
    "class CNNWithTabular(nn.Module):\n",
    "    def __init__(self, image_out_dim, tabular_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(3),\n",
    "            nn.Conv2d(16, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(3),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(3),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128 * 8 * 8, image_out_dim), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.tabular_net = nn.Sequential(\n",
    "            nn.Linear(tabular_dim, 512), nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128), nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.tab_scale = nn.Parameter(torch.ones(1) * 1.5)  \n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(image_out_dim + 128, 512), nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256), nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, tab):\n",
    "        img_out = self.cnn(img)\n",
    "        tab_out = self.tabular_net(tab)\n",
    "        tab_out = tab_out * self.tab_scale\n",
    "        x = torch.cat([img_out, tab_out], dim=1)\n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f3332f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 1th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0938200000025776-funda.jpg'\n",
      "Dropped 2th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0984200000013978-funda.jpg'\n",
      "Dropped 3th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0917200000025392-funda.jpg'\n",
      "Dropped 4th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0957200000019190-funda.jpg'\n",
      "Dropped 5th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0988200000035883-funda.jpg'\n",
      "Dropped 6th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0988200000049049-funda.jpg'\n",
      "Dropped 7th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0907200000015447-funda.jpg'\n",
      "Dropped 8th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0994200002032196-funda-in-business.jpg'\n",
      "Dropped 9th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0995200000758886-funda.jpg'\n",
      "Dropped 10th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0935200000069177-google.jpg'\n",
      "Dropped 11th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0951200000003611-funda.jpg'\n",
      "Dropped 12th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0962200000220209-funda.jpg'\n",
      "Dropped 13th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0951200000007702-funda.jpg'\n",
      "Dropped 14th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0935200000012468-funda.jpg'\n",
      "Dropped 15th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0984200000015956-funda.jpg'\n",
      "Dropped 16th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/17/1719200000010702-funda-in-business.jpg'\n",
      "Dropped 17th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/01/0153200000416262-google.jpg'\n",
      "Dropped 18th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0935200000003013-funda.jpg'\n",
      "Dropped 19th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0983200000027005-funda.jpg'\n",
      "Dropped 20th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0983200000019253-funda.jpg'\n",
      "Dropped 21th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0917200000052035-funda.jpg'\n",
      "Dropped 22th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0935200000012780-funda.jpg'\n",
      "Dropped 23th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0935200000099401-funda.jpg'\n",
      "Dropped 24th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0983200000013371-funda.jpg'\n",
      "Dropped 25th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0917200000025828-funda-in-business.jpg'\n",
      "Dropped 26th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0988200000052262-funda.jpg'\n",
      "Dropped 27th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0957200000015084-funda.jpg'\n",
      "Dropped 28th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0935200000056979-funda.jpg'\n",
      "Dropped 29th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0938200000022777-funda.jpg'\n",
      "Dropped 30th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0935200000019682-funda.jpg'\n",
      "Dropped 31th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0928200000026189-funda.jpg'\n",
      "Dropped 32th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/03/0363200012086458-funda-in-business.jpg'\n",
      "Dropped 33th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0907200000015381-funda.jpg'\n",
      "Dropped 34th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0907200000010023-funda.jpg'\n",
      "Dropped 35th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0995200000799640-funda.jpg'\n",
      "Dropped 36th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0917200000053968-funda.jpg'\n",
      "Dropped 37th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0983200000021636-funda.jpg'\n",
      "Dropped 38th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0946200000226352-funda.jpg'\n",
      "Dropped 39th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0983200000089957-funda-in-business.jpg'\n",
      "Dropped 40th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0995200000747696-funda.jpg'\n",
      "Dropped 41th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0983200000016139-funda.jpg'\n",
      "Dropped 42th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0938200000029472-funda.jpg'\n",
      "Dropped 43th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0936200000013023-funda.jpg'\n",
      "Dropped 44th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0935200000098914-funda.jpg'\n",
      "Dropped 45th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0917200000032748-funda.jpg'\n",
      "Dropped 46th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0935200000058871-funda.jpg'\n",
      "Dropped 47th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/00/0050200000375093-google.jpg'\n",
      "Dropped 48th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0988200000036275-funda.jpg'\n",
      "Dropped 49th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0917200000035394-funda.jpg'\n",
      "Dropped 50th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0928200000031194-funda.jpg'\n",
      "Dropped 51th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0986200000505565-funda.jpg'\n",
      "Dropped 52th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0957200000011907-funda.jpg'\n",
      "Dropped 53th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0988200000050605-funda.jpg'\n",
      "Dropped 54th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0935200000040677-funda.jpg'\n",
      "Dropped 55th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/02/0269200000005631-funda-in-business.jpg'\n",
      "Dropped 56th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0928200000019797-funda.jpg'\n",
      "Dropped 57th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0962200000211687-funda.jpg'\n",
      "Dropped 58th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0917200000032282-funda.jpg'\n",
      "Dropped 59th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/00/0063200000004457-funda-in-business.jpg'\n",
      "Dropped 1th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0994200002024176-funda.jpg'\n",
      "Dropped 2th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0917200000013282-funda.jpg'\n",
      "Dropped 3th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0928200000017201-funda.jpg'\n",
      "Dropped 4th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/03/0398200000016487-funda-in-business.jpg'\n",
      "Dropped 5th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0983200000013270-funda.jpg'\n",
      "Dropped 6th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0988200000051031-funda.jpg'\n",
      "Dropped 7th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0981200000074415-funda.jpg'\n",
      "Dropped 8th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0957200000023836-funda.jpg'\n",
      "Dropped 9th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/03/0381200000110480-funda-in-business.jpg'\n",
      "Dropped 10th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0928200000028419-funda.jpg'\n",
      "Dropped 11th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0971200000046451-funda.jpg'\n",
      "Dropped 1th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0917200000024196-funda.jpg'\n",
      "Dropped 2th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0935200000051363-funda.jpg'\n",
      "Dropped 3th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0995200000807820-funda.jpg'\n",
      "Dropped 4th row due to image load error: [Errno 2] No such file or directory: '../../Data/img_dataset/09/0928200000017245-funda.jpg'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 960, 1440] at entry 0 and [3, 640, 640] at entry 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m model.train()\n\u001b[32m     31\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_img\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Silas Postma\\Documents\\Maximilian\\Pangolinis\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Silas Postma\\Documents\\Maximilian\\Pangolinis\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Silas Postma\\Documents\\Maximilian\\Pangolinis\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Silas Postma\\Documents\\Maximilian\\Pangolinis\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Silas Postma\\Documents\\Maximilian\\Pangolinis\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    214\u001b[39m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Silas Postma\\Documents\\Maximilian\\Pangolinis\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Silas Postma\\Documents\\Maximilian\\Pangolinis\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    270\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001b[32m    271\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: stack expects each tensor to be equal size, but got [3, 960, 1440] at entry 0 and [3, 640, 640] at entry 2"
     ]
    }
   ],
   "source": [
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "train_df[\"woningtype_encoded\"] = le.fit_transform(train_df[target])\n",
    "val_df[\"woningtype_encoded\"] = le.transform(val_df[target])\n",
    "test_df[\"woningtype_encoded\"] = le.transform(test_df[target])\n",
    "\n",
    "# Datasets and loaders\n",
    "train_ds = MultiModalDataset(train_df)\n",
    "val_ds = MultiModalDataset(val_df)\n",
    "test_ds = MultiModalDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=32)\n",
    "test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = CNNWithTabular(image_out_dim=128, tabular_dim=len(features), output_dim=len(le.classes_)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(16):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_img, x_mask, x_tab, y_batch in train_loader:\n",
    "        x_img, x_mask, x_tab, y_batch = x_img.to(device), x_mask.to(device), x_tab.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x_img, x_tab)\n",
    "        loss = criterion(out, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    loss_history.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "all_preds, all_true, all_probs = [], [], []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_img, x_tab, y_batch in test_loader:\n",
    "        x_img, x_tab, y_batch = x_img.to(device), x_tab.to(device), y_batch.to(device)\n",
    "        out = model(x_img, x_tab)\n",
    "        probs = torch.softmax(out, dim=1)\n",
    "        preds = probs.argmax(dim=1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_true.extend(y_batch.cpu().tolist())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "print(\"\\n--- Evaluation Metrics ---\")\n",
    "accuracy = accuracy_score(all_true, all_preds)\n",
    "precision = precision_score(all_true, all_preds, average='macro', zero_division=0)\n",
    "recall = recall_score(all_true, all_preds, average='macro', zero_division=0)\n",
    "f1 = f1_score(all_true, all_preds, average='macro', zero_division=0)\n",
    "kappa = cohen_kappa_score(all_true, all_preds)\n",
    "logloss = log_loss(all_true, all_probs)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision (macro): {precision:.4f}\")\n",
    "print(f\"Recall (macro): {recall:.4f}\")\n",
    "print(f\"F1 Score (macro): {f1:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "print(f\"Log Loss: {logloss:.4f}\")\n",
    "\n",
    "# --- Classification Report ---\n",
    "print(\"\\n--- Classification Report ---\") \n",
    "print(classification_report(all_true, all_preds, target_names=le.classes_, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495930db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "cm = confusion_matrix(all_true, all_preds)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b7cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(loss_history) + 1), loss_history, marker='o')\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca7064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
