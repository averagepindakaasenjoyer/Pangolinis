{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b4f4dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely import wkt\n",
    "from shapely.affinity import translate, scale\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "file_path = os.getenv('FILE_PATH')\n",
    "\n",
    "df_sample = pd.read_parquet(file_path + \"detailed_woning_type_sample.parquet\")\n",
    "df = pd.read_csv(file_path + \"bag_image_summary.csv\", dtype=\"string\")\n",
    "df_joined = pd.merge(df_sample, df, how=\"left\", right_on=\"bag_id\", left_on=\"bag_nummeraanduidingid\")\n",
    "df_sample_with_urls = df_joined[df_joined[\"frontview_exists\"].notna()]\n",
    "\n",
    "# If you want to add the file path to the URLs, set this to True\n",
    "add_file_path_to_urls = False\n",
    "\n",
    "# Currently a funda sourced Url goes from: \n",
    "# frontview/0797/2000/0002/3888/0797200000023888.jpg\n",
    "# to: img_dataset/07/079720000002-funda.jpg\n",
    "def extract_path(url, source):\n",
    "    if pd.isna(url) or url == '' or url is None:\n",
    "        return ''\n",
    "    id = url.rstrip('/').split('/')[-1]\n",
    "    id, *_ = id.split('.')\n",
    "    m = re.match(r'(\\d{2})', id)\n",
    "    first_two_digits = m.group(1) if m else ''\n",
    "    return f\"img_dataset/{first_two_digits}/{id}-{source}.jpg\"\n",
    "\n",
    "link_cols = ['frontview_funda_url', 'frontview_google_url', 'frontview_funda_in_business_url']\n",
    "link_sources = ['funda', 'google', 'funda-in-business'] # Sources are in file name, so need to be added to filename for correct name\n",
    "\n",
    "for col, source in zip(link_cols, link_sources):\n",
    "    df_sample_with_urls[f'{col}_split'] = df_sample_with_urls[col].map(lambda url: extract_path(url, source))\n",
    "\n",
    "# If you want to add the file path to the URLs, set add_file_path_to_urls to True\n",
    "if add_file_path_to_urls:\n",
    "    df_sample_with_urls[[f'{col}_split' for col in link_cols]] = df_sample_with_urls[[f'{col}_split' for col in link_cols]].map(lambda x: file_path + x if x else '')\n",
    "    add_file_path_to_urls = False\n",
    "\n",
    "df_sample_with_urls.to_csv(\n",
    "    file_path + \"Full_preprocessed_detailed_house.csv\",\n",
    "    index=False,\n",
    "    encoding='utf-8',\n",
    ")\n",
    "\n",
    "df = pd.read_csv(file_path + \"Full_preprocessed_detailed_house.csv\", dtype=\"string\")\n",
    "\n",
    "# Verschillend is a special case, so we remove it from the dataset\n",
    "df = df[df['build_type'] != 'Verschillend']\n",
    "\n",
    "def pick_first_url(row):\n",
    "    for col in [f\"{c}_split\" for c in link_cols]:\n",
    "        val = row[col]\n",
    "        if pd.notna(val) and val != '':\n",
    "            return val\n",
    "    return ''\n",
    "\n",
    "df['frontview_url'] = df.apply(pick_first_url, axis=1)\n",
    "df = df[df['frontview_url'] != '']\n",
    "\n",
    "# Ensure 'opp_pand' and 'oppervlakte' are numeric before division\n",
    "df['procent_ingenomen'] = pd.to_numeric(df['opp_pand'], errors='coerce') / pd.to_numeric(df['oppervlakte'], errors='coerce')\n",
    "\n",
    "df['huisnr_bag_letter'] = df['huisnr_bag_letter'].notna().astype(int)\n",
    "df['huisnr_bag_toevoeging'] = df['huisnr_bag_toevoeging'].notna().astype(int)\n",
    "\n",
    "df['is_monument'] = df['is_monument'].fillna(0).astype(int)\n",
    "df['is_protected'] = df['is_protected'].fillna(0).astype(int)\n",
    "\n",
    "df = df.drop(columns=['bag_nummeraanduidingid', 'frontview_exists', 'random_rank', 'num_funda_images',\n",
    "                      'frontview_funda_url', 'frontview_google_url', 'frontview_funda_in_business_url', \n",
    "                      'frontview_funda_url_split', 'frontview_google_url_split', 'frontview_funda_in_business_url_split',\n",
    "                      'special_house_type', 'source_data_result_id', 'source_data_timestamp',\n",
    "                      'straatnaam', 'postcode', 'plaatsnaam', 'oppervlakte',\n",
    "                      ])\n",
    "\n",
    "merge_map = {\n",
    "    'Bovenwoning': 'Bovenwoning/Benedenwoning/Maisonette',\n",
    "    'Benedenwoning': 'Bovenwoning/Benedenwoning/Maisonette',\n",
    "    'Maisonnette': 'Bovenwoning/Benedenwoning/Maisonette',\n",
    "    'Corridorflat': 'Corridorflat/Galerijflat',\n",
    "    'Galerijflat': 'Corridorflat/Galerijflat',\n",
    "    'Hoekwoning': 'Hoekwoning/Eindwoning',\n",
    "    'Eindwoning': 'Hoekwoning/Eindwoning',\n",
    "    'Portiekflat': 'Portiekflat/Portiekwoning',\n",
    "    'Portiekwoning': 'Portiekflat/Portiekwoning'\n",
    "\n",
    "    # etc.\n",
    "}\n",
    "\n",
    "df['woningtype'] = df['woningtype'].map(merge_map).fillna(df['woningtype'])\n",
    "\n",
    "# Full preprocessed dataset with URLS, can be loaded into pipeline.\n",
    "df.to_csv(\n",
    "    file_path + \"Full_preprocessed_detailed_house.csv\",\n",
    "    index=False,\n",
    "    encoding='utf-8',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e25336f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_orientation(polygon):\n",
    "    if polygon.is_empty or not polygon.is_valid:\n",
    "        return np.nan\n",
    "\n",
    "    # Get minimum rotated rectangle\n",
    "    mrr = polygon.minimum_rotated_rectangle\n",
    "    coords = list(mrr.exterior.coords)\n",
    "\n",
    "    # Find the longest edge\n",
    "    max_length = 0\n",
    "    angle = 0\n",
    "\n",
    "    for i in range(len(coords) - 1):\n",
    "        p1 = coords[i]\n",
    "        p2 = coords[i + 1]\n",
    "\n",
    "        dx = p2[0] - p1[0]\n",
    "        dy = p2[1] - p1[1]\n",
    "\n",
    "        length = np.hypot(dx, dy)\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "            angle = np.degrees(np.arctan2(dy, dx))\n",
    "\n",
    "    # Normalize angle to 0â€“180\n",
    "    return angle % 180\n",
    "\n",
    "def compute_elongation(polygon):\n",
    "    if polygon.is_empty or not polygon.is_valid:\n",
    "        return np.nan\n",
    "\n",
    "    # Minimum rotated rectangle (oriented bounding box)\n",
    "    min_rect = polygon.minimum_rotated_rectangle\n",
    "\n",
    "    # Get corner points of the box\n",
    "    coords = list(min_rect.exterior.coords)\n",
    "\n",
    "    # Compute distances between the 4 sides\n",
    "    edge_lengths = [np.linalg.norm(np.subtract(coords[i], coords[i + 1])) for i in range(4)]\n",
    "\n",
    "    width = min(edge_lengths)\n",
    "    height = max(edge_lengths)\n",
    "\n",
    "    if height == 0:  # Prevent divide-by-zero\n",
    "        return np.nan\n",
    "\n",
    "    return width / height\n",
    "\n",
    "def rasterize_polygon(geom, size=224):\n",
    "    bounds = geom.bounds\n",
    "    geom = translate(geom, xoff=-bounds[0], yoff=-bounds[1])\n",
    "    scale_x = size / (bounds[2] - bounds[0] + 1e-8)\n",
    "    scale_y = size / (bounds[3] - bounds[1] + 1e-8)\n",
    "    geom = scale(geom, xfact=scale_x, yfact=scale_y, origin=(0, 0))\n",
    "\n",
    "    img = Image.new(\"L\", (size, size), 0)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    coords = [(x, size - y) for x, y in geom.exterior.coords]\n",
    "    draw.polygon(coords, outline=1, fill=1)\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8706f689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woningtype                       object\n",
      "huisnr                   string[python]\n",
      "huisnr_bag_letter                 int64\n",
      "huisnr_bag_toevoeging             int64\n",
      "opp_pand                 string[python]\n",
      "build_year               string[python]\n",
      "build_type               string[python]\n",
      "is_monument                       int64\n",
      "is_protected                      int64\n",
      "geometry                 string[python]\n",
      "bag_id                   string[python]\n",
      "frontview_url                    object\n",
      "procent_ingenomen               Float64\n",
      "geometry_wkt                     object\n",
      "centroid_x                      float64\n",
      "centroid_y                      float64\n",
      "area                            float64\n",
      "perimeter                       float64\n",
      "compactness                     float64\n",
      "num_vertices                      int64\n",
      "elongation                      float64\n",
      "orientation_deg                 float64\n",
      "num_vertices_log                float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['geometry_wkt'] = df['geometry'].apply(wkt.loads)\n",
    "\n",
    "df['centroid_x'] = df['geometry_wkt'].apply(lambda geom: geom.centroid.x)\n",
    "df['centroid_y'] = df['geometry_wkt'].apply(lambda geom: geom.centroid.y)\n",
    "\n",
    "df['area'] = df['geometry_wkt'].apply(lambda geom: geom.area)\n",
    "\n",
    "# perimeter = Sum of the lengths of all edges forming the boundary of a polygon\n",
    "df['perimeter'] = df['geometry_wkt'].apply(lambda g: g.length)\n",
    "\n",
    "# Gives 1 for a perfect circle (most compact shape)\n",
    "# Gets closer to 0 for long, skinny, jagged shapes\n",
    "df['compactness'] = (\n",
    "    4 * np.pi * df['area'] / (df['perimeter'] ** 2)\n",
    ")\n",
    "\n",
    "df['num_vertices'] = df['geometry_wkt'].apply(lambda g: len(g.exterior.coords))\n",
    "\n",
    "df['elongation'] = df['geometry_wkt'].apply(compute_elongation)\n",
    "\n",
    "df['orientation_deg'] = df['geometry_wkt'].apply(compute_orientation)\n",
    "\n",
    "df['num_vertices_log'] = np.log1p(df['num_vertices'])\n",
    "\n",
    "df.to_csv(\n",
    "    file_path + \"Full_preprocessed_detailed_house.csv\",\n",
    "    index=False,\n",
    "    encoding='utf-8',\n",
    ")\n",
    "\n",
    "print(df.dtypes)\n",
    "\n",
    "df[\"mask\"] = df[\"geometry_wkt\"].apply(lambda g: rasterize_polygon(g, size=224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "803b9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing and feature engineering are applied to the dataset separately for training, validation, and testing.\n",
    "# This is to ensure that the model does not learn from the validation and test sets during training.\n",
    "\n",
    "# adjust random_state for reproducibility\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "numeric_cols = ['opp_pand', 'build_year', 'huisnr', 'procent_ingenomen']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_df[numeric_cols] = scaler.fit_transform(train_df[numeric_cols])\n",
    "val_df[numeric_cols] = scaler.transform(val_df[numeric_cols])\n",
    "test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\n",
    "\n",
    "\n",
    "for col in ['centroid_x', 'centroid_y']:\n",
    "    min_val = train_df[col].min()\n",
    "    max_val = train_df[col].max()\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df[col] = (df[col] - min_val) / (max_val - min_val + 1e-8)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoder.fit(train_df[['build_type']])\n",
    "feature_names = [name.replace(' ', '_') for name in encoder.get_feature_names_out(['build_type'])]\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    encoded_array = encoder.transform(df[['build_type']])\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=feature_names, index=df.index)\n",
    "    df.drop(columns='build_type', inplace=True)\n",
    "    df[encoded_df.columns] = encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82f81024",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'huisnr',                                   # 0 - inf\n",
    "    'huisnr_bag_letter',                        # 0 - 1\n",
    "    'huisnr_bag_toevoeging',                    # 0 - 1\n",
    "    'opp_pand',                                 # StandardScaler\n",
    "    'oppervlakte',                              # StandardScaler\n",
    "    'build_year',                               # StandardScaler\n",
    "    'build_type_Appartement',                   # OneHotEncoder\n",
    "    'build_type_Hoekwoning',                    # OneHotEncoder\n",
    "    'build_type_Tussen_of_geschakelde_woning',  # OneHotEncoder\n",
    "    'build_type_Tweeonder1kap',                 # OneHotEncoder\n",
    "    'build_type_Verschillend',                  # OneHotEncoder\n",
    "    'is_monument',                              # 0 - 1\n",
    "    'is_protected',                             # 0 - 1\n",
    "    'procent_ingenomen',                        # 0 - 1\n",
    "    'centroid_x',                               #  \n",
    "    'centroid_y',                               # \n",
    "    'perimeter',                                # \n",
    "    'compactness',                              # \n",
    "    'elongation',                               # \n",
    "    'orientation_deg',                          # \n",
    "    'num_vertices_log'                          # \n",
    "]\n",
    "\n",
    "target = 'woningtype'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38fa180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
