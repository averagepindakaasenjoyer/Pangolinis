{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb7eef1",
   "metadata": {},
   "source": [
    "# Multimodal Housing Classification Pipeline\n",
    "\n",
    "This notebook implements a multimodal deep learning pipeline to classify housing types based on both image data and tabular features.\n",
    "\n",
    "Currently it uses an example Hybrid-model, this part will be exchanged for other models.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Section 1: Setup and Environment](#section-1-setup-and-environment)  \n",
    "- [Section 2: Data Ingestion](#section-2-data-ingestion)  \n",
    "- [Section 3: Preprocessing and Splitting](#section-3-preprocessing-and-splitting)  \n",
    "- [Section 4: Dataset and DataLoader](#section-4-dataset-and-dataloader)  \n",
    "- [Section 5: Multimodal Model Definition](#section-5-multimodal-model-definition)  \n",
    "- [Section 6: Training & Validation Functions](#section-6-training--validation-functions)  \n",
    "- [Section 7: Evaluation & Model Utilities](#section-7-evaluation--model-utilities)  \n",
    "- [Section 8: Pipeline Execution](#section-8-pipeline-execution)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b2aacd",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Environment\n",
    "\n",
    "- Load necessary libraries for data handling, preprocessing, modeling, and visualization.\n",
    "- Configure device (GPU/CPU).\n",
    "- Load environment variables for paths (ensure `.env` file contains `FILE_PATH`).\n",
    "- Define image directory and CSV data path.\n",
    "\n",
    "Make sure you have the `.env` file properly configured with the base file path to your dataset and images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be620149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Base Directory: ../../../Samsung_USB/\n",
      "Data CSV Path: ../../../Samsung_USB/Full_preprocessed_detailed_house.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Section 1: Setup ---\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    cohen_kappa_score, \n",
    "    log_loss, \n",
    "    classification_report, \n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Environment and Path Configuration\n",
    "load_dotenv()\n",
    "BASE_DIR = os.getenv('FILE_PATH')\n",
    "if not BASE_DIR:\n",
    "    raise ValueError(\"FILE_PATH environment variable not set. Please create a .env file and set it.\")\n",
    "\n",
    "DATA_PATH = os.path.join(BASE_DIR, 'Full_preprocessed_detailed_house.csv')\n",
    "print(f\"Base Directory: {BASE_DIR}\")\n",
    "print(f\"Data CSV Path: {DATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39cfbe5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025d05e",
   "metadata": {},
   "source": [
    "## Section 2: Data Ingestion\n",
    "\n",
    "- Load the dataset CSV file into a DataFrame.\n",
    "- Print confirmation and dataset shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72661b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data. Shape: (6595, 23)\n"
     ]
    }
   ],
   "source": [
    "# --- Section 2: Data Ingestion ---\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Successfully loaded data. Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7af4de8",
   "metadata": {},
   "source": [
    "## Section 3: Preprocessing and Splitting\n",
    "\n",
    "- Stratified splitting of dataset into train (60%), validation (20%), and test (20%) sets based on the target label (`woningtype`).\n",
    "- Scale numeric features using training set statistics.\n",
    "- One-hot encode categorical features using training set statistics.\n",
    "- Clean and convert features to numeric types and handle missing values.\n",
    "- Create full image paths and encode target labels with `LabelEncoder`.\n",
    "- Define final tabular features for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1da07d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train (60%), validation (20%), and test (20%)...\n",
      "Numeric features scaled.\n",
      "Categorical features one-hot encoded.\n",
      "Total tabular features (7): ['opp_pand', 'build_year', 'build_type_Appartement', 'build_type_Hoekwoning', 'build_type_Tussen of geschakelde woning', 'build_type_Tweeonder1kap', 'build_type_Vrijstaande woning']\n",
      "Cleaning and converting features to numeric types...\n",
      "Preparing final data (image paths and labels)...\n",
      "Train size: 3957 | Val size: 1319 | Test size: 1319\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "# --- Section 3: Preprocessing and Splitting ---\n",
    "\n",
    "def preprocess_tabular_data(train_df, val_df, test_df):\n",
    "    \"\"\"Scales numeric columns and one-hot encodes categorical columns.\"\"\"\n",
    "    numeric_cols = ['opp_pand', 'build_year']\n",
    "    categorical_cols = ['build_type']\n",
    "    \n",
    "    # Scale numeric columns based on the training set\n",
    "    scaler = StandardScaler()\n",
    "    train_df[numeric_cols] = scaler.fit_transform(train_df[numeric_cols])\n",
    "    val_df[numeric_cols] = scaler.transform(val_df[numeric_cols])\n",
    "    test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\n",
    "    print(\"Numeric features scaled.\")\n",
    "\n",
    "    for col in ['centroid_x', 'centroid_y']:\n",
    "        min_val = train_df[col].min()\n",
    "        max_val = train_df[col].max()\n",
    "        for df in [train_df, val_df, test_df]:\n",
    "            df[col] = (df[col] - min_val) / (max_val - min_val + 1e-8)\n",
    "\n",
    "    # One-hot encode categorical columns based on the training set\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    encoder.fit(train_df[categorical_cols])\n",
    "    \n",
    "    cat_encoded_cols = list(encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "    def encode_and_merge(df, encoder):\n",
    "        encoded_data = encoder.transform(df[categorical_cols])\n",
    "        encoded_df = pd.DataFrame(encoded_data, columns=cat_encoded_cols, index=df.index)\n",
    "        return pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "\n",
    "    train_df = encode_and_merge(train_df, encoder)\n",
    "    val_df = encode_and_merge(val_df, encoder)\n",
    "    test_df = encode_and_merge(test_df, encoder)\n",
    "    print(\"Categorical features one-hot encoded.\")\n",
    "    \n",
    "    return train_df, val_df, test_df, numeric_cols, cat_encoded_cols\n",
    "\n",
    "\n",
    "def clean_and_convert_features(df, feature_list):\n",
    "    \"\"\"Ensures all specified feature columns are numeric, filling NaNs.\"\"\"\n",
    "    cleaned_df = df.copy()\n",
    "    for feature in feature_list:\n",
    "        if feature in cleaned_df.columns:\n",
    "            # Convert to numeric, coercing errors to NaN\n",
    "            cleaned_df[feature] = pd.to_numeric(cleaned_df[feature], errors='coerce')\n",
    "            # Fill any resulting NaNs with 0 (or a more suitable value like the mean)\n",
    "            if cleaned_df[feature].isnull().sum() > 0:\n",
    "                cleaned_df[feature] = cleaned_df[feature].fillna(0)\n",
    "            # Ensure final type is float32 for PyTorch\n",
    "            cleaned_df[feature] = cleaned_df[feature].astype('float32')\n",
    "    return cleaned_df\n",
    "\n",
    "def prepare_final_data(df, base_path):\n",
    "    \"\"\"Adds the full image path and encodes the 'woningtype' label.\"\"\"\n",
    "    # Build full image path\n",
    "    df['img_path'] = df['frontview_url'].apply(lambda x: os.path.join(base_path, x))\n",
    "    return df\n",
    "\n",
    "# 1. Split the original dataframe\n",
    "print(\"Splitting data into train (60%), validation (20%), and test (20%)...\")\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42, stratify=df['woningtype'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['woningtype'])\n",
    "\n",
    "# 2. Preprocess tabular data (scaling and encoding)\n",
    "train_df, val_df, test_df, numeric_cols, cat_encoded_cols = preprocess_tabular_data(train_df, val_df, test_df)\n",
    "\n",
    "# 3. Define the full list of features for the model\n",
    "tabular_features = numeric_cols + cat_encoded_cols\n",
    "print(f\"Total tabular features ({len(tabular_features)}): {tabular_features}\")\n",
    "\n",
    "# 4. Clean the dataframes to ensure all feature columns are numeric\n",
    "print(\"Cleaning and converting features to numeric types...\")\n",
    "train_df = clean_and_convert_features(train_df, tabular_features)\n",
    "val_df = clean_and_convert_features(val_df, tabular_features)\n",
    "test_df = clean_and_convert_features(test_df, tabular_features)\n",
    "\n",
    "# 5. Create image paths and encode labels\n",
    "print(\"Preparing final data (image paths and labels)...\")\n",
    "train_df = prepare_final_data(train_df, BASE_DIR)\n",
    "val_df = prepare_final_data(val_df, BASE_DIR)\n",
    "test_df = prepare_final_data(test_df, BASE_DIR)\n",
    "\n",
    "# Fit LabelEncoder on the full training 'woningtype' to create integer labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['label'] = label_encoder.fit_transform(train_df['woningtype'])\n",
    "# Use the same encoder for validation and test sets\n",
    "val_df['label'] = label_encoder.transform(val_df['woningtype'])\n",
    "test_df['label'] = label_encoder.transform(test_df['woningtype'])\n",
    "\n",
    "print(f\"Train size: {len(train_df)} | Val size: {len(val_df)} | Test size: {len(test_df)}\")\n",
    "print(f\"Number of classes: {len(label_encoder.classes_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8757925b",
   "metadata": {},
   "source": [
    "## Section 4: Dataset and DataLoader\n",
    "\n",
    "- Custom PyTorch `Dataset` class loads images and tabular features on-the-fly.\n",
    "- Applies data augmentations only on training data (random horizontal flips and rotations) (SUBJECT TO CHANGE).\n",
    "- Uses ImageNet normalization to match pretrained model input expectations.\n",
    "- DataLoaders created with batch size of 32 and multi-threading support (adjust `NUM_WORKERS` accordingly).\n",
    "\n",
    "**Notes:**\n",
    "- You can adjust batch size and transforms to fit your hardware and data augmentation preferences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c100b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets and DataLoaders created successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Section 4: Dataset and DataLoader ---\n",
    "\n",
    "class HousingDataset(Dataset):\n",
    "    def __init__(self, df, tabular_features, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.tabular_features = tabular_features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        try:\n",
    "            # Image processing\n",
    "            img = Image.open(row['img_path']).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            \n",
    "            v = row[self.tabular_features].values.astype(np.float32)\n",
    "            tab_feats = torch.from_numpy(v)\n",
    "            label = torch.tensor(row['label'], dtype=torch.long)\n",
    "            \n",
    "            return img, tab_feats, label\n",
    "            \n",
    "        except:\n",
    "            # print(f\"Error loading data at index {idx} (path: {row['img_path']}). Skipping. Error: {e}\")\n",
    "            # Return the next valid sample to avoid crashing the loader\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "# ImageNet normalization values, commonly used for pretrained models\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(), # Add some augmentation (OPTIONEEL)\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "# Initialize datasets\n",
    "train_dataset = HousingDataset(train_df, tabular_features, transform=train_transforms)\n",
    "val_dataset = HousingDataset(val_df, tabular_features, transform=val_transforms)\n",
    "test_dataset = HousingDataset(test_df, tabular_features, transform=val_transforms)\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 32 # Adjusted for common GPU memory sizes, can be tuned\n",
    "# Set num_workers=0 on Windows or for debugging, can be > 0 on Linux for performance\n",
    "NUM_WORKERS = 0 if os.name == 'nt' else 2 \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(\"Datasets and DataLoaders created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5449ec",
   "metadata": {},
   "source": [
    "## Section 5: Multimodal Model Definition\n",
    "\n",
    "### ZET HIER JE MODEL\n",
    "\n",
    "### Currently: \n",
    "- CNN backbone: ResNet18 pretrained on ImageNet, outputs 512-dim image features.\n",
    "- Tabular MLP: Two-layer fully connected network with batch normalization and dropout.\n",
    "- Fusion layer concatenates image and tabular embeddings before final classification head.\n",
    "\n",
    "**Parameters to tune:**\n",
    "- `tabular_input_dim`: Number of tabular features.\n",
    "- Dropout rates and hidden layer sizes can be modified for regularization and capacity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "783ea244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Section 5: Model Definition ---\n",
    "\n",
    "class TabularModel(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP for tabular features matching `tabular_emb_dim`.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, emb_dim=128, hidden_dim=256, dropout=0.3):\n",
    "        super(TabularModel, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, emb_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_tab):\n",
    "        return self.mlp(x_tab)\n",
    "\n",
    "\n",
    "class ImageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet18 backbone producing a feature vector of size `cnn_output_dim`.\n",
    "    \"\"\"\n",
    "    def __init__(self, cnn_output_dim=512, pretrained=True):\n",
    "        super(ImageModel, self).__init__()\n",
    "        weights = models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        resnet = models.resnet18(weights=weights)\n",
    "        # drop the final fc layer and avgpool\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.cnn_output_dim = cnn_output_dim\n",
    "\n",
    "    def forward(self, x_img):\n",
    "        # x_img: [B,3,H,W]\n",
    "        feat = self.backbone(x_img)               # [B, cnn_output_dim, 1, 1]\n",
    "        return feat.view(feat.size(0), self.cnn_output_dim)\n",
    "\n",
    "class ResNet50Classifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.base_model = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Freeze base model\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Replace the final fully connected layer\n",
    "        in_features = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Identity()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class ResNet50FeatureExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.base_model = models.resnet50(pretrained=pretrained)\n",
    "\n",
    "        # Remove the final fully connected layer\n",
    "        self.base_model.fc = nn.Identity()\n",
    "        self.output_dim = 2048  # This is the output dimension of resnet50 without the fc layer\n",
    "\n",
    "        # Optionally freeze weights\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "\n",
    "class EarlyFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Early fusion classifier:\n",
    "    Args:\n",
    "        tabular_input_dim (int): number of tabular features\n",
    "        num_classes (int): number of output classes\n",
    "        cnn_output_dim (int): image embedding size\n",
    "        tabular_emb_dim (int): tabular embedding size\n",
    "        dropout (float): dropout rate\n",
    "        pretrained (bool): use imagenet weights\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 tabular_input_dim,\n",
    "                 num_classes,\n",
    "                 cnn_output_dim=512,  # <-- changed from 512 to 2048 for ResNet50\n",
    "                 tabular_emb_dim=128,\n",
    "                 dropout=0.5,\n",
    "                 pretrained=True):\n",
    "        super(EarlyFusionModel, self).__init__()\n",
    "        self.img_model = ImageModel(cnn_output_dim=cnn_output_dim, pretrained=pretrained)\n",
    "        self.tab_model = TabularModel(input_dim=tabular_input_dim,\n",
    "                                      emb_dim=tabular_emb_dim,\n",
    "                                      hidden_dim=256,\n",
    "                                      dropout=dropout)\n",
    "\n",
    "        fusion_dim = cnn_output_dim + tabular_emb_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, tabular_data):\n",
    "        img_feat = self.img_model(image)\n",
    "        tab_feat = self.tab_model(tabular_data)\n",
    "        fused = torch.cat([img_feat, tab_feat], dim=1)\n",
    "        return self.classifier(fused)\n",
    "\n",
    "\n",
    "class LateFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Late fusion classifier combining separate heads:\n",
    "    Args:\n",
    "        tabular_input_dim (int)\n",
    "        num_classes (int)\n",
    "        cnn_output_dim (int)\n",
    "        tabular_emb_dim (int)\n",
    "        dropout (float)\n",
    "        pretrained (bool)\n",
    "        fusion_method (str): 'concat' or 'weighted'\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 tabular_input_dim,\n",
    "                 num_classes,\n",
    "                 cnn_output_dim=512,  # <-- changed from 512 to 2048 for ResNet50\n",
    "                 tabular_emb_dim=128,\n",
    "                 dropout=0.5,\n",
    "                 pretrained=True,\n",
    "                 fusion_method='concat'):\n",
    "        super(LateFusionModel, self).__init__()\n",
    "        # image branch\n",
    "        self.img_model = ImageModel(cnn_output_dim=cnn_output_dim, pretrained=pretrained)\n",
    "        self.img_clf = nn.Sequential(\n",
    "            nn.Linear(cnn_output_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        # tabular branch\n",
    "        self.tab_model = TabularModel(input_dim=tabular_input_dim,\n",
    "                                      emb_dim=tabular_emb_dim,\n",
    "                                      hidden_dim=256,\n",
    "                                      dropout=dropout)\n",
    "        self.tab_clf = nn.Linear(tabular_emb_dim, num_classes)\n",
    "\n",
    "        self.fusion_method = fusion_method\n",
    "        if fusion_method == 'concat':\n",
    "            self.fusion_clf = nn.Linear(num_classes * 2, num_classes)\n",
    "        elif fusion_method == 'weighted':\n",
    "            self.img_w = nn.Parameter(torch.tensor(0.5))\n",
    "            self.tab_w = nn.Parameter(torch.tensor(0.5))\n",
    "        else:\n",
    "            raise ValueError(\"fusion_method must be 'concat' or 'weighted'\")\n",
    "\n",
    "    def forward(self, image, tabular_data):\n",
    "        img_feat = self.img_model(image)\n",
    "        img_logits = self.img_clf(img_feat)\n",
    "\n",
    "        tab_feat = self.tab_model(tabular_data)\n",
    "        tab_logits = self.tab_clf(tab_feat)\n",
    "\n",
    "        if self.fusion_method == 'concat':\n",
    "            combined = torch.cat([img_logits, tab_logits], dim=1)\n",
    "            return self.fusion_clf(combined)\n",
    "        else:\n",
    "            w_img = torch.sigmoid(self.img_w)\n",
    "            w_tab = torch.sigmoid(self.tab_w)\n",
    "            return w_img * img_logits + w_tab * tab_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2b2bfb",
   "metadata": {},
   "source": [
    "## Section 6: Training & Validation Functions\n",
    "\n",
    "This section defines functions for performing one epoch of training and validation for the multimodal model.\n",
    "\n",
    "- **train_one_epoch**:  \n",
    "  Executes a single epoch of training. The model is set to training mode.  \n",
    "  For each batch:\n",
    "  - Data is moved to the specified device (CPU/GPU).\n",
    "  - Forward pass is performed to compute outputs.\n",
    "  - Cross-entropy loss is computed and backpropagated.\n",
    "  - Model parameters are updated via the optimizer.\n",
    "  - Running loss and predictions are tracked for metrics calculation.\n",
    "  \n",
    "  Returns the average loss, accuracy, and weighted F1 score over the entire training dataset for the epoch.\n",
    "\n",
    "- **validate_one_epoch**:  \n",
    "  Executes a single epoch of validation without updating model parameters. The model is set to evaluation mode and gradient calculations are disabled.  \n",
    "  For each batch:\n",
    "  - Data is moved to the device.\n",
    "  - Forward pass is performed and loss computed.\n",
    "  - Predictions and losses are collected.\n",
    "  \n",
    "  Returns the average loss, accuracy, and weighted F1 score over the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f12f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Section 6: Training & Validation Functions ---\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    loop = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for images, tabular_data, labels in loop:\n",
    "        images, tabular_data, labels = images.to(device), tabular_data.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, tabular_data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    return epoch_loss, epoch_acc, epoch_f1\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    loop = tqdm(dataloader, desc=\"Validating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, tabular_data, labels in loop:\n",
    "            images, tabular_data, labels = images.to(device), tabular_data.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images, tabular_data)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    return epoch_loss, epoch_acc, epoch_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cffe251",
   "metadata": {},
   "source": [
    "## Section 7: Evaluation & Model Utilities\n",
    "\n",
    "This section contains utility functions for evaluating the model’s performance, saving, and loading the model weights.\n",
    "\n",
    "- **evaluate_model**:  \n",
    "  Runs inference on a dataset and prints a detailed classification report along with a confusion matrix heatmap to visualize performance across classes.\n",
    "\n",
    "- **save_model**:  \n",
    "  Saves the model’s state dictionary to a `models/` directory. If the directory does not exist, it will be created automatically.  \n",
    "  To avoid overwriting, if a model file with the given name already exists, a numeric suffix (`_1`, `_2`, etc.) is appended to the filename.\n",
    "\n",
    "- **load_model**:  \n",
    "  Loads the saved model weights from a specified path, moves the model to the appropriate device (CPU or GPU), and sets it to evaluation mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44f4889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Section 7: Evaluation & Model Utilities ---\n",
    "\n",
    "def evaluate_model(model, dataloader, device, class_names):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a given dataloader and prints a comprehensive report\n",
    "    including individual metrics, a classification report, and a confusion matrix.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, tabular_data, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            tabular_data = tabular_data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images, tabular_data)\n",
    "            \n",
    "            # Get probabilities for log_loss\n",
    "            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "            all_probs.extend(probs)\n",
    "            \n",
    "            # Get predictions for other metrics\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # --- Calculate and Print Individual Metrics ---\n",
    "    print(\"\\n--- Evaluation Metrics ---\")\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    kappa = cohen_kappa_score(all_labels, all_preds)\n",
    "    logloss = log_loss(all_labels, all_probs)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision (macro): {precision:.4f}\")\n",
    "    print(f\"Recall (macro): {recall:.4f}\")\n",
    "    print(f\"F1 Score (macro): {f1:.4f}\")\n",
    "    print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "    print(f\"Log Loss: {logloss:.4f}\")\n",
    "\n",
    "    # --- Classification Report ---\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names, zero_division=0))\n",
    "\n",
    "    # --- Confusion Matrix ---\n",
    "    print(\"\\n--- Confusion Matrix ---\")\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model, filename='trained_model.pth'):\n",
    "    models_dir = 'models'\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    base_path = os.path.join(models_dir, filename)\n",
    "\n",
    "    # If file exists, add suffix\n",
    "    if os.path.exists(base_path):\n",
    "        base_name, ext = os.path.splitext(filename)\n",
    "        suffix = 1\n",
    "        while True:\n",
    "            new_filename = f\"{base_name}_{suffix}{ext}\"\n",
    "            new_path = os.path.join(models_dir, new_filename)\n",
    "            if not os.path.exists(new_path):\n",
    "                base_path = new_path\n",
    "                break\n",
    "            suffix += 1\n",
    "\n",
    "    torch.save(model.state_dict(), base_path)\n",
    "    print(f\"Model saved to {base_path}\")\n",
    "    return base_path\n",
    "\n",
    "\n",
    "\n",
    "def load_model(model_architecture, path, device):\n",
    "    model_architecture.load_state_dict(torch.load(path, map_location=device))\n",
    "    model_architecture.to(device)\n",
    "    model_architecture.eval()\n",
    "    print(f\"Model loaded from {path} and set to evaluation mode.\")\n",
    "    return model_architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d68bb2",
   "metadata": {},
   "source": [
    "## Section 8: Pipeline Execution\n",
    "\n",
    "- Initialize model, criterion (cross entropy), and optimizer (Adam). (SUBJECT TO CHANGE)\n",
    "- Run training/validation for specified epochs.\n",
    "- Save best model weights.\n",
    "- Load best model for final testing and evaluation.\n",
    "\n",
    "**User Options:**\n",
    "- Modify `NUM_EPOCHS`, learning rate (`lr`), and batch size (`BATCH_SIZE`) to fit your needs.\n",
    "- You can replace `MultimodalHousingClassifier` with your custom model if desired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca083773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting full pipeline execution...\n",
      "Model initialized with 11,379,146 trainable parameters.\n",
      "Training for 5 epochs...\n",
      "\n",
      "--- Epoch 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Summary:\n",
      "  Train -> Loss: 1.8810, Acc: 0.3192, F1: 0.3183\n",
      "  Valid -> Loss: 1.5345, Acc: 0.4473, F1: 0.4471\n",
      "Model saved to models\\best_housing_classifier_3.pth\n",
      "🎉 New best model saved with F1 score: 0.4471\n",
      "\n",
      "--- Epoch 2/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Summary:\n",
      "  Train -> Loss: 1.3219, Acc: 0.5342, F1: 0.5220\n",
      "  Valid -> Loss: 1.2552, Acc: 0.5489, F1: 0.5335\n",
      "Model saved to models\\best_housing_classifier_4.pth\n",
      "🎉 New best model saved with F1 score: 0.5335\n",
      "\n",
      "--- Epoch 3/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 115/124 [01:23<00:06,  1.30it/s, loss=1.02] "
     ]
    }
   ],
   "source": [
    "# --- Section 8: Main Pipeline Execution ---\n",
    "print(\"🚀 Starting full pipeline execution...\")\n",
    "\n",
    "# 1. Initialize Model\n",
    "tabular_input_dim = len(tabular_features)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = EarlyFusionModel(tabular_input_dim=tabular_input_dim, num_classes=num_classes, cnn_output_dim=512).to(device)\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.\")\n",
    "\n",
    "# 2. Define Loss, Optimizer, and Hyperparameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "NUM_EPOCHS = 5 # You can adjust this\n",
    "best_val_f1 = 0\n",
    "BEST_MODEL_PATH = \"best_housing_classifier.pth\"\n",
    "\n",
    "print(f\"Training for {NUM_EPOCHS} epochs...\")\n",
    "\n",
    "# 3. Run Training and Validation Loop\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n--- Epoch {epoch}/{NUM_EPOCHS} ---\")\n",
    "    \n",
    "    train_loss, train_acc, train_f1 = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch} Summary:\")\n",
    "    print(f\"  Train -> Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
    "    print(f\"  Valid -> Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Save the best model based on validation F1 score\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        model_path = save_model(model, BEST_MODEL_PATH)\n",
    "        print(f\"🎉 New best model saved with F1 score: {best_val_f1:.4f}\")\n",
    "\n",
    "print(\"\\n✅ Training complete.\")\n",
    "\n",
    "# 4. Evaluate the Best Model on the Test Set\n",
    "print(f\"\\n📊 Loading best model from '{model_path}' and evaluating on the test set...\")\n",
    "# Re-initialize a fresh model architecture\n",
    "final_model = EarlyFusionModel(tabular_input_dim=tabular_input_dim, num_classes=num_classes, cnn_output_dim=2048)\n",
    "# Load the saved state dict\n",
    "final_model = load_model(final_model, model_path, device)\n",
    "\n",
    "\n",
    "class_names = list(label_encoder.classes_)\n",
    "evaluate_model(final_model, test_loader, device, class_names=class_names)\n",
    "\n",
    "print(\"\\n✅ Pipeline finished successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a6db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting full pipeline execution...\n",
      "Model initialized with 11,741,094 trainable parameters.\n",
      "Training for 5 epochs...\n",
      "\n",
      "--- Epoch 1/5 ---\n",
      "Model initialized with 11,741,094 trainable parameters.\n",
      "Training for 5 epochs...\n",
      "\n",
      "--- Epoch 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 2048]' is invalid for input of size 16384",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NUM_EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m     train_loss, train_acc, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     val_loss, val_acc, val_f1 \u001b[38;5;241m=\u001b[39m validate_one_epoch(model, val_loader, criterion, device)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[53], line 13\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     10\u001b[0m images, tabular_data, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), tabular_data\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtabular_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[52], line 169\u001b[0m, in \u001b[0;36mLateFusionModel.forward\u001b[1;34m(self, image, tabular_data)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, tabular_data):\n\u001b[1;32m--> 169\u001b[0m     img_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     img_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_clf(img_feat)\n\u001b[0;32m    172\u001b[0m     tab_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtab_model(tabular_data)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[52], line 37\u001b[0m, in \u001b[0;36mImageModel.forward\u001b[1;34m(self, x_img)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_img):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# x_img: [B,3,H,W]\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(x_img)               \u001b[38;5;66;03m# [B, cnn_output_dim, 1, 1]\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn_output_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[32, 2048]' is invalid for input of size 16384"
     ]
    }
   ],
   "source": [
    "# --- Section 8: Main Pipeline Execution ---\n",
    "print(\"🚀 Starting full pipeline execution...\")\n",
    "\n",
    "# 1. Initialize Model\n",
    "tabular_input_dim = len(tabular_features)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = LateFusionModel(tabular_input_dim=tabular_input_dim, num_classes=num_classes, cnn_output_dim=2048).to(device)\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.\")\n",
    "\n",
    "# 2. Define Loss, Optimizer, and Hyperparameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "NUM_EPOCHS = 5 # You can adjust this\n",
    "best_val_f1 = 0\n",
    "BEST_MODEL_PATH = \"best_housing_classifier.pth\"\n",
    "\n",
    "print(f\"Training for {NUM_EPOCHS} epochs...\")\n",
    "\n",
    "# 3. Run Training and Validation Loop\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n--- Epoch {epoch}/{NUM_EPOCHS} ---\")\n",
    "    \n",
    "    train_loss, train_acc, train_f1 = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch} Summary:\")\n",
    "    print(f\"  Train -> Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
    "    print(f\"  Valid -> Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Save the best model based on validation F1 score\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        model_path = save_model(model, BEST_MODEL_PATH)\n",
    "        print(f\"🎉 New best model saved with F1 score: {best_val_f1:.4f}\")\n",
    "\n",
    "print(\"\\n✅ Training complete.\")\n",
    "\n",
    "# 4. Evaluate the Best Model on the Test Set\n",
    "print(f\"\\n📊 Loading best model from '{model_path}' and evaluating on the test set...\")\n",
    "# Re-initialize a fresh model architecture\n",
    "final_model = LateFusionModel(tabular_input_dim=tabular_input_dim, num_classes=num_classes, cnn_output_dim=2048)\n",
    "# Load the saved state dict\n",
    "final_model = load_model(final_model, model_path, device)\n",
    "\n",
    "\n",
    "class_names = list(label_encoder.classes_)\n",
    "evaluate_model(final_model, test_loader, device, class_names=class_names)\n",
    "\n",
    "print(\"\\n✅ Pipeline finished successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
