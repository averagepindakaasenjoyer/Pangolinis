{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "file_path = '../Desktop/Matrix/DATA/'\n",
    "\n",
    "df_sample = pd.read_parquet(file_path + \"detailed_woning_type_sample.parquet\")\n",
    "df = pd.read_csv(file_path + \"bag_image_summary.csv\", dtype=\"string\")\n",
    "df_joined = pd.merge(df_sample, df, how=\"left\", right_on=\"bag_id\", left_on=\"bag_nummeraanduidingid\")\n",
    "df_sample_with_urls = df_joined[df_joined[\"frontview_exists\"].notna()]\n",
    "\n",
    "# If you want to add the file path to the URLs, set this to True\n",
    "add_file_path_to_urls = False\n",
    "\n",
    "# Currently a funda sourced Url goes from: \n",
    "# frontview/0797/2000/0002/3888/0797200000023888.jpg\n",
    "# to: img_dataset/07/079720000002-funda.jpg\n",
    "def extract_path(url, source):\n",
    "    if pd.isna(url) or url == '' or url is None:\n",
    "        return ''\n",
    "    id = url.rstrip('/').split('/')[-1]\n",
    "    id, *_ = id.split('.')\n",
    "    m = re.match(r'(\\d{2})', id)\n",
    "    first_two_digits = m.group(1) if m else ''\n",
    "    return f\"img_dataset/{first_two_digits}/{id}-{source}.jpg\"\n",
    "\n",
    "link_cols = ['frontview_funda_url', 'frontview_google_url', 'frontview_funda_in_business_url']\n",
    "link_sources = ['funda', 'google', 'funda-in-business'] # Sources are in file name, so need to be added to filename for correct name\n",
    "\n",
    "for col, source in zip(link_cols, link_sources):\n",
    "    df_sample_with_urls[f'{col}_split'] = df_sample_with_urls[col].map(lambda url: extract_path(url, source))\n",
    "\n",
    "# If you want to add the file path to the URLs, set add_file_path_to_urls to True\n",
    "if add_file_path_to_urls:\n",
    "    df_sample_with_urls[[f'{col}_split' for col in link_cols]] = df_sample_with_urls[[f'{col}_split' for col in link_cols]].map(lambda x: file_path + x if x else '')\n",
    "    add_file_path_to_urls = False\n",
    "\n",
    "df_sample_with_urls.to_csv(\n",
    "    file_path + \"Full_preprocessed_detailed_house.csv\",\n",
    "    index=False,\n",
    "    encoding='utf-8',\n",
    ")\n",
    "\n",
    "df = pd.read_csv(file_path + \"Full_preprocessed_detailed_house.csv\", dtype=\"string\")\n",
    "\n",
    "# Verschillend is a special case, so we remove it from the dataset\n",
    "df = df[df['build_type'] != 'Verschillend']\n",
    "\n",
    "def pick_first_url(row):\n",
    "    for col in [f\"{c}_split\" for c in link_cols]:\n",
    "        val = row[col]\n",
    "        if pd.notna(val) and val != '':\n",
    "            return val\n",
    "    return ''\n",
    "\n",
    "df['frontview_url'] = df.apply(pick_first_url, axis=1)\n",
    "df = df[df['frontview_url'] != '']\n",
    "\n",
    "# Ensure 'opp_pand' and 'oppervlakte' are numeric before division\n",
    "df['procent_ingenomen'] = pd.to_numeric(df['opp_pand'], errors='coerce') / pd.to_numeric(df['oppervlakte'], errors='coerce')\n",
    "\n",
    "df['huisnr_bag_letter'] = df['huisnr_bag_letter'].notna().astype(int)\n",
    "df['huisnr_bag_toevoeging'] = df['huisnr_bag_toevoeging'].notna().astype(int)\n",
    "\n",
    "df['is_monument'] = df['is_monument'].fillna(0).astype(int)\n",
    "df['is_protected'] = df['is_protected'].fillna(0).astype(int)\n",
    "\n",
    "df = df.drop(columns=['bag_nummeraanduidingid', 'frontview_exists', 'random_rank', 'num_funda_images',\n",
    "                      'frontview_funda_url', 'frontview_google_url', 'frontview_funda_in_business_url', \n",
    "                      'frontview_funda_url_split', 'frontview_google_url_split', 'frontview_funda_in_business_url_split',\n",
    "                      'special_house_type', 'source_data_result_id',\n",
    "                      ])\n",
    "\n",
    "# CURRENTLY DELETING GEOMETRY COLUMN, CHANGE IF NEEDED\n",
    "df = df.drop(columns=['geometry'])\n",
    "\n",
    "\n",
    "# Full preprocessed dataset with URLS, can be loaded into pipeline.\n",
    "df.to_csv(\n",
    "    file_path + \"Full_preprocessed_detailed_house.csv\",\n",
    "    index=False,\n",
    "    encoding='utf-8',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing and feature engineering are applied to the dataset separately for training, validation, and testing.\n",
    "# This is to ensure that the model does not learn from the validation and test sets during training.\n",
    "\n",
    "# adjust random_state for reproducibility\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "for dataframe in [train_df, val_df, test_df]:\n",
    "    for col in ['opp_pand', 'oppervlakte', 'build_year']:\n",
    "        scaler = StandardScaler()\n",
    "        dataframe[col] = scaler.fit_transform(dataframe[[col]])\n",
    "\n",
    "    if dataframe is train_df:\n",
    "        build_type_train = train_df[['build_type']]\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoder.fit(build_type_train)\n",
    "        raw_feature_names = encoder.get_feature_names_out(['build_type'])\n",
    "        clean_feature_names = [name.replace(' ', '_') for name in raw_feature_names]\n",
    "\n",
    "    build_type = dataframe[['build_type']]\n",
    "    encoded_array = encoder.transform(build_type)\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=clean_feature_names, index=dataframe.index)\n",
    "    dataframe.drop('build_type', axis=1, inplace=True)\n",
    "    dataframe[encoded_df.columns] = encoded_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "straatnaam                                 string[python]\n",
      "postcode                                   string[python]\n",
      "huisnr                                     string[python]\n",
      "huisnr_bag_letter                                   int64\n",
      "huisnr_bag_toevoeging                               int64\n",
      "plaatsnaam                                 string[python]\n",
      "opp_pand                                          float64\n",
      "oppervlakte                                       float64\n",
      "build_year                                        float64\n",
      "is_monument                                         int64\n",
      "is_protected                                        int64\n",
      "source_data_timestamp                      string[python]\n",
      "bag_id                                     string[python]\n",
      "frontview_url                                      object\n",
      "procent_ingenomen                                 Float64\n",
      "build_type_Appartement                            float64\n",
      "build_type_Hoekwoning                             float64\n",
      "build_type_Tussen_of_geschakelde_woning           float64\n",
      "build_type_Tweeonder1kap                          float64\n",
      "build_type_Vrijstaande_woning                     float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#wow moeilijke code (niet)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#guassian NB ipv categorical omdat onze data niet alleen categorical is (WOW)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m NB \u001b[38;5;241m=\u001b[39m GaussianNB()\n\u001b[0;32m---> 18\u001b[0m \u001b[43mNB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/naive_bayes.py:263\u001b[0m, in \u001b[0;36mGaussianNB.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m\"\"\"Fit Gaussian Naive Bayes according to X, y.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03m    Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    262\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(y\u001b[38;5;241m=\u001b[39my)\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partial_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_refit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/naive_bayes.py:431\u001b[0m, in \u001b[0;36mGaussianNB._partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    425\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# If the ratio of data variance between dimensions is too small, it\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# will cause numerical errors. To address this, we artificially\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# boost the variance by epsilon, a small fraction of the standard\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# deviation of the largest dimension.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_smoothing \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_call:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;66;03m# This is the first call to partial_fit:\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# initialize various cumulative counters\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:4008\u001b[0m, in \u001b[0;36mvar\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where, mean, correction)\u001b[0m\n\u001b[1;32m   4005\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4006\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m var(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, ddof\u001b[38;5;241m=\u001b[39mddof, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 4008\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4009\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/_core/_methods.py:175\u001b[0m, in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where, mean)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrmean, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n\u001b[0;32m--> 175\u001b[0m         arrmean \u001b[38;5;241m=\u001b[39m \u001b[43mum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrue_divide\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrmean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munsafe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(arrmean, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    178\u001b[0m     arrmean \u001b[38;5;241m=\u001b[39m arrmean\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(arrmean \u001b[38;5;241m/\u001b[39m rcount)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "#classical naive bayas probability model for baseline\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "\n",
    "\n",
    "X_train = train_df.drop(columns=[\"woningtype\"])\n",
    "y_train = train_df[\"woningtype\"]\n",
    "\n",
    "X_val = val_df.drop(columns=[\"woningtype\"])\n",
    "y_val = val_df[\"woningtype\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"woningtype\"])\n",
    "y_test = test_df[\"woningtype\"]\n",
    "\n",
    "#wow moeilijke code (niet)\n",
    "#guassian NB ipv categorical omdat onze data niet alleen categorical is (WOW)\n",
    "NB = GaussianNB()\n",
    "NB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "file_path = os.getenv('FILE_PATH')\n",
    "\n",
    "df_sample = pd.read_parquet(file_path + \"detailed_woning_type_sample.parquet\")\n",
    "df = pd.read_csv(file_path + \"bag_image_summary.csv\", dtype=\"string\")\n",
    "df_joined = pd.merge(df_sample, df, how=\"left\", right_on=\"bag_id\", left_on=\"bag_nummeraanduidingid\")\n",
    "df_sample_with_urls = df_joined[df_joined[\"frontview_exists\"].notna()]\n",
    "\n",
    "# If you want to add the file path to the URLs, set this to True\n",
    "add_file_path_to_urls = False\n",
    "\n",
    "# Currently a funda sourced Url goes from: \n",
    "# frontview/0797/2000/0002/3888/0797200000023888.jpg\n",
    "# to: img_dataset/07/079720000002-funda.jpg\n",
    "def extract_path(url, source):\n",
    "    if pd.isna(url) or url == '' or url is None:\n",
    "        return ''\n",
    "    id = url.rstrip('/').split('/')[-1]\n",
    "    id, *_ = id.split('.')\n",
    "    m = re.match(r'(\\d{2})', id)\n",
    "    first_two_digits = m.group(1) if m else ''\n",
    "    return f\"img_dataset/{first_two_digits}/{id}-{source}.jpg\"\n",
    "\n",
    "link_cols = ['frontview_funda_url', 'frontview_google_url', 'frontview_funda_in_business_url']\n",
    "link_sources = ['funda', 'google', 'funda-in-business'] # Sources are in file name, so need to be added to filename for correct name\n",
    "\n",
    "for col, source in zip(link_cols, link_sources):\n",
    "    df_sample_with_urls[f'{col}_split'] = df_sample_with_urls[col].map(lambda url: extract_path(url, source))\n",
    "\n",
    "# If you want to add the file path to the URLs, set add_file_path_to_urls to True\n",
    "if add_file_path_to_urls:\n",
    "    df_sample_with_urls[[f'{col}_split' for col in link_cols]] = df_sample_with_urls[[f'{col}_split' for col in link_cols]].map(lambda x: file_path + x if x else '')\n",
    "    add_file_path_to_urls = False\n",
    "\n",
    "df_sample_with_urls.to_csv(\n",
    "    file_path + \"Full_preprocessed_detailed_house.csv\",\n",
    "    index=False,\n",
    "    encoding='utf-8',\n",
    ")\n",
    "\n",
    "df = pd.read_csv(file_path + \"Full_preprocessed_detailed_house.csv\", dtype=\"string\")\n",
    "\n",
    "# Verschillend is a special case, so we remove it from the dataset\n",
    "df = df[df['build_type'] != 'Verschillend']\n",
    "\n",
    "def pick_first_url(row):\n",
    "    for col in [f\"{c}_split\" for c in link_cols]:\n",
    "        val = row[col]\n",
    "        if pd.notna(val) and val != '':\n",
    "            return val\n",
    "    return ''\n",
    "\n",
    "df['frontview_url'] = df.apply(pick_first_url, axis=1)\n",
    "df = df[df['frontview_url'] != '']\n",
    "\n",
    "# Ensure 'opp_pand' and 'oppervlakte' are numeric before division\n",
    "df['procent_ingenomen'] = pd.to_numeric(df['opp_pand'], errors='coerce') / pd.to_numeric(df['oppervlakte'], errors='coerce')\n",
    "\n",
    "df['huisnr_bag_letter'] = df['huisnr_bag_letter'].notna().astype(int)\n",
    "df['huisnr_bag_toevoeging'] = df['huisnr_bag_toevoeging'].notna().astype(int)\n",
    "\n",
    "df['is_monument'] = df['is_monument'].fillna(0).astype(int)\n",
    "df['is_protected'] = df['is_protected'].fillna(0).astype(int)\n",
    "\n",
    "df = df.drop(columns=['bag_nummeraanduidingid', 'frontview_exists', 'random_rank', 'num_funda_images',\n",
    "                      'frontview_funda_url', 'frontview_google_url', 'frontview_funda_in_business_url', \n",
    "                      'frontview_funda_url_split', 'frontview_google_url_split', 'frontview_funda_in_business_url_split',\n",
    "                      'special_house_type', 'source_data_result_id',\n",
    "                      ])\n",
    "\n",
    "# CURRENTLY DELETING GEOMETRY COLUMN, CHANGE IF NEEDED\n",
    "df = df.drop(columns=['geometry'])\n",
    "\n",
    "\n",
    "# Full preprocessed dataset with URLS, can be loaded into pipeline.\n",
    "df.to_csv(\n",
    "    file_path + \"Full_preprocessed_detailed_house.csv\",\n",
    "    index=False,\n",
    "    encoding='utf-8',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing and feature engineering are applied to the dataset separately for training, validation, and testing.\n",
    "# This is to ensure that the model does not learn from the validation and test sets during training.\n",
    "\n",
    "# adjust random_state for reproducibility\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "for dataframe in [train_df, val_df, test_df]:\n",
    "    for col in ['opp_pand', 'oppervlakte', 'build_year']:\n",
    "        scaler = StandardScaler()\n",
    "        dataframe[col] = scaler.fit_transform(dataframe[[col]])\n",
    "\n",
    "    if dataframe is train_df:\n",
    "        build_type_train = train_df[['build_type']]\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoder.fit(build_type_train)\n",
    "        raw_feature_names = encoder.get_feature_names_out(['build_type'])\n",
    "        clean_feature_names = [name.replace(' ', '_') for name in raw_feature_names]\n",
    "\n",
    "    build_type = dataframe[['build_type']]\n",
    "    encoded_array = encoder.transform(build_type)\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=clean_feature_names, index=dataframe.index)\n",
    "    dataframe.drop('build_type', axis=1, inplace=True)\n",
    "    dataframe[encoded_df.columns] = encoded_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwoningtype\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     10\u001b[0m y_train \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwoningtype\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     12\u001b[0m X_val \u001b[38;5;241m=\u001b[39m val_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwoningtype\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "#discriminitive classical model (probabky xgboost or random forest)\n",
    "#random forest is denk beter als deze modellen alleen als baseline worden gebruikt\n",
    "#xg is meer accurate tho\n",
    "#heb de categorical features niet geencode want dacht dat koen dat al gedaan had? anders ga ik er nog achteraan\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train = train_df.drop(columns=[\"woningtype\"])\n",
    "y_train = train_df[\"woningtype\"]\n",
    "\n",
    "X_val = val_df.drop(columns=[\"woningtype\"])\n",
    "y_val = val_df[\"woningtype\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"woningtype\"])\n",
    "y_test = test_df[\"woningtype\"]\n",
    "\n",
    "#random forest is minder accuraat maar meer interpeteerbaar en minder complex\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#accurater maar moet ff meer hyperparamter tuningg doen\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "#Ik heb nog even niets aangeraakt voor het valideren want weet nog niet welke metric we gebruiken\n",
    "#wil ook niet onze test zet verneuken en leake ofzo\n",
    "#moet nog parameters tunen maar wilde de pipeline hiervoor gebruiken denk ik?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "        accuracy_score, precision_score, recall_score, f1_score,\n",
    "        log_loss, cohen_kappa_score, \n",
    "    )\n",
    "\n",
    "def evaluate_model(model, dataloader, device, class_names=None):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, tabular_data, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            tabular_data = tabular_data.to(device)\n",
    "            labels = labels.to(device)\n",
    "    \n",
    "            outputs = model(images, tabular_data)\n",
    "            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "            all_probs.extend(probs)\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    kappa = cohen_kappa_score(all_labels, all_preds)\n",
    "    logloss = log_loss(all_labels, all_probs)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision (macro): {precision:.4f}\")\n",
    "    print(f\"Recall (macro): {recall:.4f}\")\n",
    "    print(f\"F1 Score (macro): {f1:.4f}\")\n",
    "    print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "    print(f\"Log Loss: {logloss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
